Index: apache-nutch-1.3/src/java/org/apache/nutch/crawl/AbstractFetchSchedule.java
===================================================================
--- apache-nutch-1.3.orig/src/java/org/apache/nutch/crawl/AbstractFetchSchedule.java
+++ apache-nutch-1.3/src/java/org/apache/nutch/crawl/AbstractFetchSchedule.java
@@ -185,8 +185,8 @@ public abstract class AbstractFetchSched
       datum.setFetchInterval(maxInterval * 0.9f);
     datum.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);
     datum.setRetriesSinceFetch(0);
-    datum.setSignature(null);
-    datum.setModifiedTime(0L);
+    //datum.setSignature(null);
+    //datum.setModifiedTime(0L);
     if (asap) datum.setFetchTime(System.currentTimeMillis());
     return datum;
   }
Index: apache-nutch-1.3/src/java/org/apache/nutch/crawl/CrawlDb.java
===================================================================
--- apache-nutch-1.3.orig/src/java/org/apache/nutch/crawl/CrawlDb.java
+++ apache-nutch-1.3/src/java/org/apache/nutch/crawl/CrawlDb.java
@@ -49,6 +49,9 @@ public class CrawlDb extends Configured
   public static final String CURRENT_NAME = "current";
   
   public static final String LOCK_NAME = ".locked";
+
+  public static final String CRAWLDB_NEW_LINKS = "db.update.new.links";
+  private boolean newLinks = false;
   
   public CrawlDb() {}
   
@@ -74,10 +77,12 @@ public class CrawlDb extends Configured
       LOG.info("CrawlDb update: additions allowed: " + additionsAllowed);
       LOG.info("CrawlDb update: URL normalizing: " + normalize);
       LOG.info("CrawlDb update: URL filtering: " + filter);
+      LOG.info("CrawlDb update: Forcing fetch of new links: " + newLinks);
     }
 
     JobConf job = CrawlDb.createJob(getConf(), crawlDb);
     job.setBoolean(CRAWLDB_ADDITIONS_ALLOWED, additionsAllowed);
+    job.setBoolean(CRAWLDB_NEW_LINKS, newLinks);
     job.setBoolean(CrawlDbFilter.URL_FILTERING, filter);
     job.setBoolean(CrawlDbFilter.URL_NORMALIZING, normalize);
     for (int i = 0; i < segments.length; i++) {
@@ -166,6 +171,7 @@ public class CrawlDb extends Configured
       System.err.println("\t-normalize\tuse URLNormalizer on urls in CrawlDb and segment (usually not needed)");
       System.err.println("\t-filter\tuse URLFilters on urls in CrawlDb and segment");
       System.err.println("\t-noAdditions\tonly update already existing URLs, don't add any newly discovered URLs");
+      System.err.println("\t-newLinks\tforce refetching of all new links");
       return -1;
     }
     boolean normalize = false;
@@ -183,6 +189,8 @@ public class CrawlDb extends Configured
         force = true;
       } else if (args[i].equals("-noAdditions")) {
         additionsAllowed = false;
+      } else if (args[i].equals("-newLinks")) {
+        newLinks = true;
       } else if (args[i].equals("-dir")) {
         FileStatus[] paths = fs.listStatus(new Path(args[++i]), HadoopFSUtil.getPassDirectoriesFilter(fs));
         dirs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));
Index: apache-nutch-1.3/src/java/org/apache/nutch/crawl/CrawlDbReducer.java
===================================================================
--- apache-nutch-1.3.orig/src/java/org/apache/nutch/crawl/CrawlDbReducer.java
+++ apache-nutch-1.3/src/java/org/apache/nutch/crawl/CrawlDbReducer.java
@@ -43,6 +43,7 @@ public class CrawlDbReducer implements R
   private InlinkPriorityQueue linked = null;
   private ScoringFilters scfilters = null;
   private boolean additionsAllowed;
+  private boolean newLinks;
   private int maxInterval;
   private FetchSchedule schedule;
 
@@ -50,6 +51,7 @@ public class CrawlDbReducer implements R
     retryMax = job.getInt("db.fetch.retry.max", 3);
     scfilters = new ScoringFilters(job);
     additionsAllowed = job.getBoolean(CrawlDb.CRAWLDB_ADDITIONS_ALLOWED, true);
+    newLinks = job.getBoolean(CrawlDb.CRAWLDB_NEW_LINKS, false);
     int oldMaxInterval = job.getInt("db.max.fetch.interval", 0);
     maxInterval = job.getInt("db.fetch.interval.max", 0 );
     if (oldMaxInterval > 0 && maxInterval == 0) maxInterval = oldMaxInterval * FetchSchedule.SECONDS_PER_DAY;
@@ -182,6 +184,8 @@ public class CrawlDbReducer implements R
     case CrawlDatum.STATUS_LINKED:                // it was link
       if (oldSet) {                          // if old exists
         result.set(old);                          // use it
+	if (newLinks)
+	  result = schedule.forceRefetch(key, result, true);
       } else {
         result = schedule.initializeSchedule((Text)key, result);
         result.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);
